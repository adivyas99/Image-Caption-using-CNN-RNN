{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax, array\n",
    "from pickle import load, dump\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.models import load_model, Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make some important functions-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For loading the image input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IMAGE_TEXT_LOAD(FNAME):\n",
    "    \n",
    "\tfile = open(FNAME, 'r')\n",
    "\tdoc = file.read()\n",
    "\tfile.close()\n",
    "    \n",
    "\tDATA = list()\n",
    "    \n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\tif len(line) < 1: # Not including empty lines\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\tidentifier = line.split('.')[0] # Take the unique image Splited by a DOT.\n",
    "\t\tDATA.append(identifier)\n",
    "        \n",
    "\treturn set(DATA) #returning the list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding sequence tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MATCH_DESC_ADD_TOKEN(FNAME, DATA):\n",
    "    \n",
    "\tfile = open(FNAME, 'r')\n",
    "\tdoc = file.read()\n",
    "\tfile.close()\n",
    "    \n",
    "\tREFINED_DESC = dict()\n",
    "    \n",
    "\tfor line in doc.split('\\n'):\n",
    "        \n",
    "\t\ttokens = line.split() # split line by every space taking each word\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:] # Taking ID from description\n",
    "        \n",
    "\t\t# skip images not in the description set\n",
    "\t\tif image_id in DATA:\n",
    "\t\t\tif image_id not in REFINED_DESC:\n",
    "\t\t\t\tREFINED_DESC[image_id] = list() #empty list where its not present\n",
    "                \n",
    "\t\t\t# Adding tokens in description with relevent images\n",
    "\t\t\tdesc = 'SOS ' + ' '.join(image_desc) + ' EOS' # SOS= START_OF_SEQUENCE | EOS=END_OF SEQUENCE\n",
    "\t\t\tREFINED_DESC[image_id].append(desc) # appending it to the dictionary\n",
    "            \n",
    "\treturn REFINED_DESC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making dictionary to Tokenise (and its reverse) and Max Sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WORD_TO_ID(descriptions):\n",
    "    # Tokenize words given in caption descriptions\n",
    "\tlines = list()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[lines.append(d) for d in descriptions[key]]\n",
    "\t\n",
    "\ttokenizer = Tokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "def ID_TO_WORD(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None\n",
    "\n",
    "def MAX_SEQUENCE_LENGTH(descriptions):\n",
    "    lines = list()\n",
    "    for key in descriptions.keys():\n",
    "        [lines.append(d) for d in descriptions[key]]\n",
    "        \n",
    "    return max(len(d.split()) for d in lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For making description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CREATE_DESCRIPTION(model, tokenizer, photo, max_length):\n",
    "\n",
    "\tin_text = 'SOS' # Start the generation process\n",
    "\tfor i in range(max_length): # iterating over whole sequence\n",
    "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0] # integer encoding\n",
    "\t\tsequence = pad_sequences([sequence], maxlen=max_length) # padding input\n",
    "\t\tyhat = model.predict([photo,sequence], verbose=0) # predicting next word\n",
    "\t\tyhat = argmax(yhat) # converting probability to integer\n",
    "\t\tword = ID_TO_WORD(yhat, tokenizer) # map token to word\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\tin_text += ' ' + word # append as input for creating next word\n",
    "\t\tif word == 'EOS': # stop if we forecast the EOS\n",
    "\t\t\tbreak\n",
    "        \n",
    "\treturn in_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For making sequences of I/P's and O/P's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAKE_SEQUENCES(tokenizer, max_length, descriptions, photos):\n",
    "    # Make I/P sequences of text and image sequences and O/P words for an image\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\t\n",
    "\tfor key, desc_list in descriptions.items(): # Itirate over each image \n",
    "\t\t\n",
    "\t\tfor desc in desc_list: # Go through each description for an image\n",
    "\t\t\t\n",
    "\t\t\tseq = tokenizer.texts_to_sequences([desc])[0] # encoding sequence\n",
    "\t\t\t\n",
    "\t\t\tfor i in range(1, len(seq)): #Spliting to I/P and O/P\n",
    "\t\t\t\t\n",
    "\t\t\t\tin_seq, out_seq = seq[:i], seq[i] # Make a pair\n",
    "\t\t\t\t\n",
    "\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0] # Input Padding\n",
    "\t\t\t\t\n",
    "\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0] # Encode O/P\n",
    "\t\t\t\t\n",
    "                # Appending it\n",
    "\t\t\t\tX1.append(photos[key][0])\n",
    "\t\t\t\tX2.append(in_seq)\n",
    "\t\t\t\ty.append(out_seq)\n",
    "                \n",
    "\treturn array(X1), array(X2), array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets define our CNN and RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MODEL_DEFINITION(vocab_size, max_length):\n",
    "\t# TAKING BOTH INPUTS--\n",
    "    # For Image\n",
    "\tinputs1 = Input(shape=(4096,))\n",
    "\tfe1 = Dropout(0.5)(inputs1)\n",
    "\tfe2 = Dense(256, activation='relu')(fe1)\n",
    "    \n",
    "\t# For Text - Sequence model\n",
    "\tinputs2 = Input(shape=(max_length,))\n",
    "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "\tse2 = Dropout(0.5)(se1)\n",
    "\tse3 = LSTM(256)(se2)\n",
    "    \n",
    "\t# Decoder Part\n",
    "\tdecoder1 = add([fe2, se3])\n",
    "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
    "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    \n",
    "\t# tie all and Feeding to NN\n",
    "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    \n",
    "    # Compiling the Model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For generating sets of I/O used while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GENERATE_DATA(descriptions, photos, tokenizer, max_length):\n",
    "\t\n",
    "\twhile 1: # Itirate  over every image \n",
    "\t\tfor key, desc_list in descriptions.items():\n",
    "            \n",
    "\t\t\t# Get Image features\n",
    "\t\t\tphoto = photos[key][0]\n",
    "\t\t\tin_img, in_seq, out_word = MAKE_SEQUENCES(tokenizer, max_length, desc_list, photo)\n",
    "            \n",
    "\t\t\tyield [[in_img, in_seq], out_word]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training DATA (6K)\n",
    "FNAME1 = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = IMAGE_TEXT_LOAD(FNAME1)\n",
    "\n",
    "# descriptions\n",
    "train_descriptions = MATCH_DESC_ADD_TOKEN('descriptions.txt', train)\n",
    "# prepare tokenizer\n",
    "tokenizer = WORD_TO_ID(train_descriptions)\n",
    "\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "\n",
    "#taking training features\n",
    "Load_all_features = load(open('features.pkl', 'rb')) # taking all features given in Binary Read Mode\n",
    "train_features = {k: Load_all_features[k] for k in train}\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# determine the maximum sequence length\n",
    "max_length = MAX_SEQUENCE_LENGTH(train_descriptions)\n",
    "\n",
    "# prepare sequences\n",
    "X1train, X2train, ytrain = MAKE_SEQUENCES(tokenizer, max_length, train_descriptions, train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "FNAME2 = 'Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "test = IMAGE_TEXT_LOAD(FNAME2)\n",
    "# descriptions\n",
    "test_descriptions = MATCH_DESC_ADD_TOKEN('descriptions.txt', test)\n",
    "\n",
    "# photo features\n",
    "# Lets see image features-\n",
    "Load_all_features = load(open('features.pkl', 'rb')) # taking all features given in Binary Read Mode\n",
    "test_features = {k: Load_all_features[k] for k in test}\n",
    "\n",
    "# prepare sequences\n",
    "X1test, X2test, ytest = MAKE_SEQUENCES(tokenizer, max_length, test_descriptions, test_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MODEL_DEFINITION(vocab_size, max_length) # Defining the Model \n",
    "\n",
    "EPOCHS = 10\n",
    "STEPS = len(train_descriptions)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "\t# Generate Data\n",
    "\tgenerator = GENERATE_DATA(train_descriptions, train_features, tokenizer, max_length)\n",
    "    \n",
    "\t# fitting for each epoch\n",
    "\tmodel.fit_generator(generator, epochs=1, steps_per_epoch=STEPS, verbose=1)\n",
    "    \n",
    "\t# saving model\n",
    "\tmodel.save('model_' + str(i) + '.h5')\n",
    "\n",
    "# load the model\n",
    "filename = 'model_09.h5'\n",
    "model = load_model(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting caption of a sample image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First get the Image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TAKE_IMAGE_FEATURES(FNAME):\n",
    "\t# load the model\n",
    "\tmodel = VGG16()\n",
    "\t\n",
    "\tmodel.layers.pop() # Modifying our model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    \n",
    "\timage = load_img(FNAME, target_size=(224, 224)) # getting the photo\n",
    "    \n",
    "\timage = img_to_array(image) # change image to a numpy array\n",
    "    \n",
    "\t# reshaping data as an input to the model\n",
    "\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    \n",
    "\timage = preprocess_input(image) # Getting the image for VGG model\n",
    "    \n",
    "\t# At last, getting features\n",
    "\tfeature = model.predict(image, verbose=0)\n",
    "    \n",
    "\treturn feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets get the captions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "# pre-set the max seq length getting after training\n",
    "max_length = 34 \n",
    "\n",
    "# load the model\n",
    "model = load_model('model_09.h5')\n",
    "\n",
    "# load and prepare the photograph\n",
    "photo = TAKE_IMAGE_FEATURES('Some_image.jpg')\n",
    "\n",
    "# generating description\n",
    "description = CREATE_DESCRIPTION(model, tokenizer, photo, max_length)\n",
    "print(description)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
